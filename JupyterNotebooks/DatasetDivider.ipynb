{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying articles according to their types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data not split yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame _init_\n",
    "news = pd.read_csv('../Datasets/news_cleaned_2018_02_13.csv')\n",
    "news.drop(columns=['Unnamed: 0','Unnamed: 17'],inplace=True)\n",
    "news.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.iloc[5]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Datasets/newsSplit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsreader = pd.read_csv(open('../Datasets/news_cleaned_2018_02_13.csv', 'rU'), chunksize = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = ['Unnamed: 0', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
    "                 'inserted_at', 'updated_at', 'title', 'authors', 'keywords','meta_keywords',\n",
    "                 'meta_description', 'tags', 'summary', 'source','Unnamed:17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_types = ['fake', 'political', 'conspiracy',\n",
    "              'junksci', 'bias', 'unreliable',\n",
    "              'unknown', 'satire', 'reliable',\n",
    "              'hate', 'clickbait', 'rumor'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because for loop looked too messy with these.\n",
    "def newsFilter(news):\n",
    "    \n",
    "    fake = news[news['type'].astype(str) == 'fake']\n",
    "    political  = news[news['type'].astype(str) == 'political']\n",
    "    conspiracy = news[news['type'].astype(str) == 'conspiracy']\n",
    "    junksci = news[news['type'].astype(str) == 'junksci']\n",
    "    bias = news[news['type'].astype(str) == 'bias']\n",
    "    unreliable = news[news['type'].astype(str) == 'unreliable']\n",
    "    unknown = news[news['type'].astype(str) == 'unknown']\n",
    "    satire = news[news['type'].astype(str) == 'satire']\n",
    "    reliable = news[news['type'].astype(str) == 'reliable']\n",
    "    hate = news[news['type'].astype(str) == 'hate']\n",
    "    clickbait = news[news['type'].astype(str) == 'clickbait']\n",
    "    rumor = news[news['type'].astype(str) == 'rumor']\n",
    "    \n",
    "    # header gets added after each iteration.\n",
    "    political.to_csv(path + 'political.csv',mode = 'a', header=False)\n",
    "    conspiracy.to_csv(path + 'conspiracy.csv',mode = 'a', header=False)\n",
    "    junksci.to_csv(path + 'junksci.csv',mode = 'a', header=False)\n",
    "    bias.to_csv(path + 'bias.csv',mode = 'a', header=False)\n",
    "    unreliable.to_csv(path + + 'unreliable.csv',mode = 'a', header=False)\n",
    "    unknown.to_csv(path + 'unknown.csv',mode = 'a', header=False)\n",
    "    satire.to_csv(path + 'satire.csv',mode = 'a', header=False)\n",
    "    reliable.to_csv(path + 'reliable.csv',mode = 'a', header=False)\n",
    "    hate.to_csv(path + 'hate.csv',mode = 'a', header=False)\n",
    "    clickbait.to_csv(path + 'clickbait.csv',mode = 'a', header=False)\n",
    "    rumor.to_csv(path + 'rumor.csv',mode = 'a', header=False)\n",
    "    fake.to_csv(path + 'fake.csv',mode = 'a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in newsreader:\n",
    "    newsFilter(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add header using terminal command -- because it's more convenient.\n",
    "subprocess.call('sed', '-i', '1i' + 'Unnamed: 0,id,domain,type,url,content,\\\n",
    "scraped_at,inserted_at,updated_at,title,authors,keywords,meta_keywords,\\\n",
    "meta_description,tags,summary,source,Unnamed:17', '*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(path + 'satire.csv')\n",
    "# x.drop(columns=['Unnamed: 0','Unnamed: 17','Unnamed: 0.1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data yet again,\n",
    "#### for more balanced and manageable chunks.\n",
    "##### Only first 10000 articles are needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mini = '../Datasets/MiniSplits/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in news_types:\n",
    "    split_reader = pd.read_csv(path + file + '.csv', chunksize=chunk_size)\n",
    "    split_reader.get_chunk(chunk_size).to_csv(path_mini + file + '_mini.csv', mode='a')\n",
    "    del split_reader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in news_types:\n",
    "    test_mini = pd.read_csv(path_mini + file + '_mini.csv')\n",
    "    print(len(test_mini.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Splitting is done here. Further cleaning needs to be performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
